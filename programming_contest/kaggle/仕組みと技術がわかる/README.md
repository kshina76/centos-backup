# 仕組みと技術がわかる教科書 書籍メモ
## 第0章 自分メモ
### やること
- オライリーの本で誤差逆伝搬の動作と誤差関数が何を入力として誤差を算出しているのかを調べる
- 第4章を飛ばしているので読んでまとめる
- 第7章以降も必要になったら学ぶ
### jupyter便利機能
- plotしたグラフはダブルクリックすると別ウィンドウで拡大したりできる
- データビューワー機能を使うと別ウィンドウでデータを表示できる
### 見るべき記事
- データ分析、機械学習手法ってたくさんあるけどいつどれを使えばよいのか
  - https://qiita.com/aokikenichi/items/688e66d10a944051039c
- 代表的な機械学習手法一覧
  - https://qiita.com/tomomoto/items/b3fd1ec7f9b68ab6dfe2
- 機械学習関連用語
  - https://qiita.com/okmttdhr/items/092ec5fc450762a73ad0
- 機械学習手法一覧とチートシート
  - https://ainow.ai/2020/06/04/222969/
- seabornでの可視化チートシートと解説
  - https://qiita.com/4m1t0/items/76b0033edb545a78cef5
- 機械学習やNNがよくまとまっている
  - https://ocw.tsukuba.ac.jp/wp/wp-content/uploads/2019/10/f779126ca7a5aa1b1d836e7e31f88f78.pdf
### 勉強の考え方
- とりあえず手を動かして結果を出してみる
- その後に理論を学ぶ
### データサイエンティストの業務フロー
- https://tjo.hatenablog.com/entry/2013/11/14/202610
### 勉強手順
- カメさんの講座で統計学などを学ぶ
- Pythonによるデータ分析入門 第2版
  - 網羅的な本
- 機械学習のための「前処理」入門
  - データの種類ごとの前処理のフローを解説
  - 借りた
- 現場のプロが伝える前処理技術 ~基礎から実践まで学ぶ
  - データの種類ごとの前処理のフローを解説
- Python実践データ分析100本ノック
  - 実践的な本
  - 借りた
- Pythonではじめる教師なし学習
  - 教師なし学習に慣れていないから
  - 借りた
- データ分析者のためのPythonデータビジュアライゼーション入門
  - データの可視化
  - 可視化をうまくまとめている
  - 借りた
- 欠測データ処理: Rによる単一代入法と多重代入法 
  - 欠損値の処理の仕方など
- 参考文献
  - https://qiita.com/aokikenichi/items/ae4df263f591e47528a6

<br></br>

## 第1章 人工知能の基礎
### 分類と回帰
- 分類は、データがどのカテゴリに属するかを調べるもの
- 回帰は、データにどのような傾向があるかを調べること

<br></br>

## 第2章 機械学習の基礎知識
### 教師あり学習
- 分類
  - 以下の2つを満たすものが分類問題に属する
    - 答えが「犬/猫」「0/1/2/3/4/5/6/7/8/9」のようにカテゴリになっていて、連続した数値ではない(離散値)
    - 大小や順序に意味がない
  - 答えが限られているのが特徴
  - 誤差関数にはクロスエントロピーがよく使われる
- 回帰
  - 以下の二つを満たすものが回帰問題に属する
    - 答えが連続した数値になる(連続値)
  - 答えにどんな値をとってもよいのが特徴
  - 誤差関数には平均二乗誤差がよく使われる
- P32: 教師あり学習のフロー
### 教師なし学習
- データの特徴を捉えることができる
- 教師なし学習の代表的なタスク
  - クラスタリング: データの中で似ている特徴ごとにグループ分けするタスク
    - 階層的クラスタリング
      - 特徴の似ているクラスタ同士を1つずつ結合させていき、1つの大きなクラスタになるまで結合する手法
    - 非階層的クラスタリング
      - 初めにクラスタ数を設定して、そのクラスタ数で最もデータを分けることができるようにクラスタリングする手法
      - k-means法とか
    - p36: わかりやすい図
  - 次元削減: データから重要な情報だけを抜き出して、それ以外を削減するタスク
    - データの可視化に使われたりする
    - p37: わかりやすい図
### 強化学習
- あとで
### 統計と機械学習の違い
- 統計
  - データを説明するのにフォーカスした分野
  - 推測の根拠が必要な場合に使用する
- 機械学習
  - データを予測するのにフォーカスした分野
  - 理由より観測性能が重要な場合に使用する

<br></br>

## 第3章 機械学習のプロセスとコア技術
### 機械学習の基本ワークフロー
1. 問題の定式化
  - どのような目標を達成したいか、そのためには何が分かればいいのかというように掘り下げていく
   - 例えば、「ネットの通販の収益を増やすには -> 商品をおすすめする -> 顧客がある商品を買う可能性を予測」
2. システム設計
  - どこからデータを取ってきて、どのように運用するかといった流れの設計
3. データの収集
4. 機械学習アルゴリズムの選択
  - 教師あり、なし、強化学習などの中から何個か
  - 良さそうなアルゴリズムをいくつかピックアップして試す
5. データの整形と特徴量エンジニアリング
  - 余分な特徴量の削除
  - 別の形式へ変換
    - 数値変数をカテゴリカル変数に変換したり
    - 文字列を数値に変換したり
  - 新たな特徴量の生成
    - 特徴量同士を組み合わせたり
    - 統計値を取ってみたり
6. モデルの学習
7. 予測結果の検証と評価
  - 精度の打ち切りラインを設定(95パーセント以上とか)して、その目標まで検証と評価を繰り返す
8. ハイパーパラメータのチューニング
  - 7の結果を受けて試す
9. システムの運用
- P58: わかりやすい画像
### データの整形
- カテゴリカルデータの整形
  - ラベルエンコーディング
    - 各カテゴリに数字を割り当てる
  - カウントエンコーディング
    - 各カテゴリの出現回数を割り当てる
  - ワンホットエンコーディング
    - 各カテゴリを0,1で表現する
  - P67: わかりやすい画像
- 数値データの整形
  - 離散化
    - 連続した値を区間に分ける方法
    - ビニングとか
  - 対数変換
    - 長い裾を短くして、小さい値を拡大できる
    - 対数変換の後に後述する標準化を行うこともある
  - スケーリング: 値の範囲を変換すること(上限がないものに上限を与えたりする)
    - Min-Maxスケーリング: 最小値0、最大値1に変換
    - 標準化: 平均0、分散1に変換
  - P69: わかりやすい画像
### 繰り返して学習(イテレーション)
- バッチ学習: 1回の学習で全ての学習データを読み込む
  - メリット
    - 全てのデータを均等に扱える
  - デメリット
    - コンピュータのメモリを圧迫する
- ミニバッチ学習: 1回の学習で「バッチサイズ」として設定した数のデータを読み込む
  - メリット
    - メモリを圧迫しにくい
  - デメリット
    - 学習結果が最後の方に読み込んだデータに引っ張られてしまうことにより、学習の順番によって性能が異なることがある
    - 異常値に弱い(正常なデータの1つとして学習してしまうから)
### オンライン学習とオフライン学習
- オフライン学習: 「モデルの学習が全て完了 -> 予測」のように学習と予測が切り離されているもの
  - 新しいデータがきたら、全てのデータを学習しなおさないといけない
  - バッチ学習
- オンライン学習: 1回の学習で「1つのデータ」を読み込む
  - 新しいデータがきたらモデルにデータを次々に投入して学習できる
  - 「学習率」というパラメータが重要
    - 大きくすると新しいデータに適用しやすくなるが、古いデータの情報が失われやすくなる
  - ミニバッチ学習
- P74,75: わかりやすい図
### テストデータによる予測結果の検証
#### データの分け方
- Train-Test分割
  - 学習データとテストデータに分ける
  - 学習データ: モデル構築、チューニングに利用
  - テストデータ: モデルの最終評価に利用

  ![train_test_tuning](https://user-images.githubusercontent.com/53253817/104553906-ca031600-567e-11eb-9080-ca1b45627420.png)

- Train-Validation-Test分割
  - 学習データ：重みの学習に使用。モデルの構築用
  - 検証データ： 学習済モデルが汎用性があるのかどうかを検証するために使用
    - ハイパーパラメータを手動でチューニングする際には、検証データに対する予測値がよくなるようにする
  - テストデータ: モデルの最終評価に利用
    - 最後の最後まで使わないから、学習・検証が終わるまでは存在を忘れてもいいレベル．

  ![train_test_validation_tuning](https://user-images.githubusercontent.com/53253817/104553908-ca9bac80-567e-11eb-8d71-a1ea76bc1d78.png)

  - 注意点
    - 検証データは検証するだけなので、重みの更新はされない
    - 検証が終わったら検証データも含めて全て学習するという方法をとると学習データが減らなくて済む
      - https://shirakonotempura.hatenablog.com/entry/2019/01/18/031645

- 参考文献
  - https://starpentagon.net/analytics/dataset_split_evaluation/
#### 学習データとテストデータの分け方(Train-Validation-Test分割した時の用語で説明する)
- ホールドアウト検証
  - 学習データとバリデーションデータを「2:1」のように割合を決める方法
  - 「2 : 1」「4 : 1」「9 : 1」がよく使用される
  - データ数が非常に多い場合に使用される
  - バリデーションデータの選び方に偏りがある場合に正確に検証できない可能性がある
- K-分割交差検証(K-foldクロスバリデーション)
  - 全てのデータがバリデーションデータになるようにパターンを変えて分割する
- Leave-one-out交差検証
  - 1データずつ抜き出してバリデーションデータとして、そのほかを学習データにする方法
  - 学習データを多くとることができるため精度の向上が期待できる
  - データ数が多いと計算量が増大するため、データ数が少ない場合に使用されることが多い
#### 誤差関数と評価関数
- 誤差関数
  - 学習の際に誤差を計算して、学習の最適化に役立てるための
  - 学習の際に最適化する対象の関数
- 評価関数
  - 学習中に検証データに対する評価値を出力するもの
  - また、学習が終わった後にテストデータに対する評価値を出力するもの
  - 評価関数は重みの更新には使われないので、評価値を目視してハイパーパラメータのチューニングを手動で行うのに使用したり、評価値を根拠としてレポートを提出したりする
  - kaggleでは評価関数が決められていて、その評価関数でいい数値をとることを競う
### 学習結果の評価方法
- 回帰モデルの性能評価
  - 出力と正答の数値の差分の「予測誤差」で評価する
  - 「予測誤差」の集計方法が色々ある
- 回帰モデルにおける代表的な予測誤差の集計方法
  - R^2(決定係数)
    - 全く予測できていない場合を0、全て予測できた場合を1として数値化する
  - RMSE(平方平均二乗誤差)
    - 予測誤差を二乗して平均して集計する
  - MAE(平均絶対誤差)
    - 予測誤差の絶対値を平均した後に集計する
- 分類モデルの性能評価
  - 混合行列という表にした後に様々な評価指数を計算することが基本
- 分類モデルの代表的な評価指数
  - 正解率
  - 再現率
  - 適合率
  - F値
### ハイパーパラメータのチューニング
- 手動だと難しい
- オートチューニング手法
  - グリッドサーチ
  - ランダムサーチ
  - 焼きなまし法(擬似アニーリング、SA)
  - ベイズ最適化
  - 遺伝的アルゴリズム
### 相関と因果
- 相関
- 因果
- 擬似相関
- P94-97をあとで読む
### フィードバックループ
- P98-100をあとで読む
- 他の記事で補完する必要がある

<br></br>

## 第4章 機械学習のアルゴリズム
### 回帰分析
#### 単回帰分析: 直線
- `y = w*x + b`
  - `w`と`b`を重みとして更新(最適化)していく
  - `x`は説明変数
  - `y`は目的変数
- 例
  - `x: 重りの重さ`、`y: バネの長さ`
- 最適化する方法
  - 最小二乗法
#### 重回帰分析: 平面
- `y = w1*x1 + w2x2 + ... + wn*xn + b`
  - `w1,w2,...,wn`と`b`を重みとして更新(最適化)していく
  - `x1,x2,...xn`は説明変数
  - `y`は目的変数
- 例
  - `x1: 店舗面積`、`x2: 駅からの近さ`、`x3: 品揃え`、`y: 売り上げ`
- 注意点
  - 多重共線性
    - 説明変数に相関の強い変数を複数入れてしまうと、正しく回帰できなくなる性質
    - 相関の高い変数が複数ある場合、1つになるように取り除く必要がある
#### 多項式回帰分析: 曲線
- `y = w1*x^1 + w2*x^2 + ... + wn*x^n + b`
  - `w1,w2,...,wn`と`b`を重みとして更新(最適化)していく
  - `x`は説明変数
  - `y`は目的変数
- 最適化する方法
  - 最小二乗法
- 注意点
  - 次数を増やすと複雑な曲線を表現できるが、増やしすぎると曲線が複雑になって過学習を起こしてしまう
#### ロバスト回帰: 外れ値の影響を小さくする方法
- RANSAC
- Theil-Sen推定量
- Huber損失
#### 過学習を抑える罰則項(正則化項)
- `x^1,x^2,...,x^300`のように説明変数が多すぎることにより回帰結果の曲線が不安定になってしまった時に行う
- 具体的な方法
  - 誤差関数を`誤差の二乗和 + 罰則項`
  - 最適化を`最小二乗法(正規化最小二乗法)`
- 上記の方法で行うと、回帰係数が大きくなるほど罰則項も大きくなる
  - 回帰係数は`w1,w2,...`といった重みのこと
- 2つの罰則項の作り方
  - L1正則化: 回帰係数の絶対値の和を基準
    - あまり重要ではない説明変数の回帰係数がゼロになる
    - 人間が見てどの変数が重要かといったことがわかるようになる
  - L2正則化: 回帰係数の2乗和を基準
    - 回帰係数を正確にゼロにすることはあまりない
    - L2の方が予測性能は高い
- 線形回帰ではL1正則化と合わせると「ラッソ回帰」といい、L2正則化と合わせると「リッジ回帰」といい、L1とL2どちらも使うと「Elastic Net回帰」という
- ニューラルネットワークでもL1正則化やL2正則化を使うと過学習を抑えられる

### サポートベクターマシン(SVM)
- 教師あり学習において、「回帰」「分類」「外れ値検出」を行う方法の1つ

<br></br>

## 第5章 ディープラーニングの基礎知識
- ニューラルネットワーク
  - パーセプトロンを積み重ねたものをニューラルネットワークという
- ドロップアウト
  - 過学習対策
  - 一定の確率でニューロンをないものとして学習すること
### 活性化関数
- 入力を重みづけした和を別の値に変形する手法
- 代表的な関数
  - シグモイド関数
  - tanh
  - ReLU
  - LeakyReLU
  - PReLU
  - SeLU
- 現在利用多く利用されているものはReLU
  - 勾配消失問題が起きにくい
    - xが0以上の場合は勾配が1になるから学習が進みやすい
    - シグモイドやtanhは勾配が0にかなり近くことにより、学習が進まない
  - デメリットはxが0より小さい場合は勾配が0になってしまうこと
- 非線形活性化関数を入れる理由(分類問題を例に)
  - 現実のデータは非線形だから
  - 線形活性化関数だと、どんなに層をディープにしても境界線が直線になってしまう
  - 非線形活性化関数だと、入力をグシャっと歪ませることができるので、複雑な分類をすることができる

<br></br>

## 第6章 ディープラーニングのプロセスとコア技術
- 順伝搬
  - データが入力から出力に流れていくこと
- 誤差逆伝搬(バックプロパゲーション)
  - 出力と正答データとの差(誤差)が後ろのノードへ伝搬するように計算を行い、その重みを調整する手法
- 誤差逆伝搬の動作
  - オライリーの本で学ぶ
### ニューラルネットワークの最適化
- 勾配を平らに近づけるように最適化する
- 代表的な最適化アルゴリズム
  - SGD(確率的勾配降下法)
  - Momentum SGD
  - Adagrad
  - RMSprop
  - Adam
### 勾配消失問題
- 入力に近いほど`w1*w2*w3*w4...`と重みが掛けられてしまい、コンピュータが表現できる値を超えてしまってゼロと扱われてしまうことによって発生する
- 対策
  - ReLUを使う
  - データの入力をバッチ正規化という方法で変換する
