# データ分析の流れ
- データ分析の手順
  - https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python
  - https://towardsdatascience.com/exploratory-data-analysis-eda-techniques-for-kaggle-competition-beginners-be4237c3c3a9
  - https://qiita.com/hkthirano/items/12e046b3e02961d8460d
  - https://data-bunseki.com/2019/08/19/kaggle%EF%BC%9Ahouse-price-%E3%83%81%E3%83%A5%E3%83%BC%E3%83%88%E3%83%AA%E3%82%A2%E3%83%ABeda%E6%8E%A2%E7%B4%A2%E7%9A%84%E3%83%87%E3%83%BC%E3%82%BF%E8%A7%A3%E6%9E%90/#i-10

- 尺度の変換方法(kaggle本)
- 時系列データの解析
- seabornのcheatsheetから有用なものを抜粋

## TOOD
- 単変量、二変量、多変量の解説と種類など色々まとまっている
  - https://www.intage.co.jp/glossary/056/
- アンサンブル、スタッキング
  - https://qiita.com/hkthirano/items/2c35a81fbc95f0e4b7c1
- ターゲット変数がcategoricalまたはnumericの場合の分析方法の違い
  - いろいろなカーネルを見るといいかも
- 時系列データのEDAからモデリング
  - https://www.kaggle.com/jagangupta/time-series-basics-exploring-traditional-ts
- データ分析系の書籍を色々まとめている人
  - https://github.com/GENZITSU/UsefulMaterials/issues?page=2&q=is%3Aissue+is%3Aopen
- 相関係数の落とし穴
  - https://www.recruit-ms.co.jp/issue/column/0000000543/
- コルモゴロフ・スミルノフ検定
  - 2つの集合が同じ分布から生まれたか否かを検定する
  - trainデータとtestデータで分布が似ていない特徴量を選択しないという手段
  - https://upura.hatenablog.com/entry/2019/03/03/233534

## 0. まずやること
### 0-1. 方針
- 単変量解析: 1~2
- 二変量解析: 3~6
- 多変量解析: 7以降
- 基本的に単変量解析と二変量解析を行ってから、多変量解析をするという手順になる
  - これは一般的なデータ分析の流れ
- 仮説を立ててからデータを見ないと思いつくものも思いつかない
- 可視化手法の選定は以下から該当するものを選ぶ
  - https://github.com/kshina76/centos-backup/blob/master/programming_contest/kaggle/%E3%83%87%E3%83%BC%E3%82%BF%E3%83%93%E3%82%B8%E3%83%A5%E3%82%A2%E3%83%A9%E3%82%A4%E3%82%BC%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3%E5%85%A5%E9%96%80/seaborn_CheatSheet.md
### 0-2. データ収集
- スクレイピングでもいいし、何かしらの方法で

### 0-3. モデルの評価指標の決定
- 評価指標が目的関数にそのまま使えるものなら、そのまま使うのがいい
- 評価指標が微分不可能なもので、別の目的関数を用意する必要があるなら、kaggle本を参照する

### 0-4. ベースのスコアを出す
1. データセットからユーザidを落とす(これはすぐやる)
2. データセットからtargetを分離する(学習する直前に落とした方がいい多分。)
3. label encodingしてGBDT(lgbm,xgb,cb)でモデル作成
4. submit(これを基準のスコアとする)

<br></br>

## 1. データ理解
### 1-1. スプレッドシートを作成して埋める
- スプレッドシートの項目
  - 変数名
  - 変数のタイプ: CategoricalまたはNumeric
  - セグメント: 変数の種類をカテゴリに分ける
    - 「築年数の変数」と「家全体のクオリティ」 -> 家セグメント
    - 特徴量を作成するときに役立つ
  - 期待: 変数の影響の期待値
    - 「高、中、低」の3段階
  - 結論: 変数の影響の結論
    - 「高、中、低」の3段階
  - コメント: 変数に対するコメント、なんでも
- 「期待」の欄の埋め方
  - 自分自身の実世界の経験に基づいて埋める
  - 例えば、木造建築と鉄筋コンクリートのどちらを好むか？とか、築年数はどれくらい気にするか？とか
- 「結論」の欄の埋め方
  - 「それぞれの変数」と「ターゲット変数」の間で散布図を描いて、結論を埋める
  - 後述する3で説明している
- このスプレッドシートを埋めることで、変数への深い理解や、いらない変数の削除に踏み切ることができる
### 1-2. 仮説を立てる
- 参考文献
  - https://custle.hatenablog.com/entry/2019/06/22/114935
#### 1-2-1. メモ書きを使った方法
1. 「メモ書きのタイトル」と「ツリーの種類」を考える(タイトルは目的変数を軸に。)
  - タイタニック -> どうすると生き残れるのか？(タイトル) -> 問題解決ツリー(How)
  - FXのレート予測 -> なぜレートは上がるのか？(タイトル) -> 原因追求ツリー(Why)
  - PUBG -> どうすれば勝てるのか？ -> 問題解決ツリー(How)
2. 決めたタイトルとツリーを使って「メモ書き」を実施する
#### 1-2-2. 5W1Hを使った方法
1. タイトルを決める(とりあえず「〜が高かった理由」のように仮定してしまっていい。タイトルは目的変数を軸に。)
  - タイタニック: タイタニックに乗っていた人の生存確率が高かった理由
  - FXのレート予測: 為替レートが上がった理由
  - PUBG: あるマッチの勝率が高かった理由
2. タイトルに対して5W1Hの視点で考えてみる(一般的に)
  - タイタニックに乗っていた人の生存確率が高かった理由
    - When:  乗ってきた時間帯によって違いはないか？/就寝時間によって違いはないか？/避難した時間によって違いはないか？
    - Where: 乗ってきた港によって違いはないか？/宿泊していた場所によって違いはないか？/事故が起きた時にいた場所によって違いはないか？
    - Who:   年齢によって違いはないか？/性別によって違いはないか？/国籍によって違いはないか？
    - What:  思いつかない
    - Why:   チケットのグレードの違いは？
    - How:   避難方法によって違いはないか？
3. タイトルに対して5W1Hの視点で考えてみる(データを見ながら)
  - データを見ながら5W1Hを適用できないか考える。適用できそうなら2のように振り分けていく
#### 1-2-3. オズボーンのチェックリストを使った方法
- 以下のオズボーンのチェックリストに沿ってデータを多面的に捉えるだけ
  - 他に使い道はないか（Put to other uses-転用）
  - 他からアイデアが借りられないか（Adapt-応用）
  - 変えてみたらどうか（Modify-変更）
  - 大きくしてみたらどうか（Magnifty-拡大）
  - 小さくしてみたらどうか（Minify-縮小）
  - 他のものでは代用できないか（Substitute-代用）
  - 入れ替えてみたらどうか（Rearrange-置換）
  - 逆にしてみたらどうか（Reverse-逆転）
  - 組み合わせてみたらどうか（Combine-結合）

### 1-3. 影響度の高そうな変数とそうでない変数に分ける
- 仮説を立てた変数に対しては後述する「2,3」で可視化する
  - 仮説が立てられなかった変数はここでは分析しない
- 全ての変数に対しては後述する「4」で可視化する
  - 仮説が立てられたものと立てられなかったものどちらも

<br></br>

## 2. 単変量解析
### 2-1. データ確認
1. データの特徴をざっと調べる
  
  ```python
  '''以下の情報を表示する
  カラム名 / カラムごとのユニーク値数 / 最も出現頻度の高い値 / 最も出現頻度の高い値の出現回数 /
  欠損損値の割合 / 最も多いカテゴリの割合 / dtypes(floatとかintとかobjectとか)
  '''
  def show_feature_describe(train):
    stats = []
    for col in train.columns:
        stats.append((col,
                    train[col].nunique(),
                    train[col].value_counts().index[0],
                    train[col].value_counts().values[0],
                    train[col].isnull().sum() * 100 / train.shape[0],
                    train[col].value_counts(normalize=True, dropna=False).values[0] * 100,
                    train[col].dtype))
    stats_df = pd.DataFrame(stats, columns=['Feature', 'Unique values', 'Most frequent item', 'Freuquence of most frequent item', 'Percentage of missing values', 'Percentage of values in the biggest category', 'Type'])
    stats_df.sort_values('Percentage of missing values', ascending=False)
    return stats_df
  ```

2. 1で調べた情報からデータタイプを推測して以下のデータタイプに振り分ける(とりあえずNumericとCategoricalでいいかも)
  - Numeric
  - Categorical/Ordinal
  - Datetime
  - Coordinates
  - 質的変数か量的変数かの判断は「質問の仕方による」
    - http://www.rikkyo.ne.jp/web/ymatsumoto/socdata09/socdata0901.pdf
  - カテゴリデータ
    - カテゴリデータは数値で測定することができないデータなので四則演算を行うことは出来ません。性別の「男性/女性」、評価などの「0:良い/1:普通/2:悪い」のようにデータの値に意味があり、分類や種類を区別するデータのこと
  - 数値データ
    - 数値によって計測、集計、分析が可能なデータのことで、数値として意味があり四則演算が可能なデータです。また時間や気温のように連続する値を連続データ、人数や個数のように1,2,3のようにとびとび、1と2の間は無いようなデータを離散データという

3. データタイプごとにカラム名をリストにまとめておく
  - あとで使うと思う
  - 一気にプロットしたいときなどに便利
    - ユニークな値が多すぎるとプロットできないので、1で調べた変数からあたりをつけておく

### 2-2. ターゲット変数の可視化: 数値変数
1. describeでありえない数字が入っていないかを最小値などを見て調べる
  
  ```python
  df_train["target"].describe()
  ```

  - 見るところ
    - マイナスを取らないはずなのに取っていないか？

2. ヒストグラムで可視化する
  
  ```python
  sns.distplot(df_train["target"])
  print(f"Skewness: {df_train["target"].skew()}")
  print(f"Kurtosis: {df_train["target"].kurt()}")
  ```

  - 見るところ
    - 正規分布からずれていないか？
    - 正の歪度か？、負の歪度か？
      - 正の歪度はヒストグラムが左にずれている。負は右
    - 尖度(ピーク)はどのくらいか？

### 2-3. ターゲット変数の可視化: カテゴリ変数
1. describeでありえない数字が入っていないかを最小値などを見て調べる
  
  ```python
  df_train["target"].describe()
  ```

  - 見るところ
    - マイナスを取らないはずなのに取っていないか？

2. カウントプロットで可視化する

  ```python
  sns.countplot(y="target", data=df_train)
  ```

  - 見るところ
    - 分布が偏っていないか？
    - 何個のクラスに分類するのか？

### 2-3. 説明変数の可視化: 数値変数
- ヒストグラム
- etc

### 2-3. 説明変数の可視化: カテゴリ変数
- 棒グラフ(カウントプロット)
- etc

- 日付データ
  - 年ごとのデータ数カウント
    - どの年にデータが集中しているかわかる
  - 月ごとのデータ数カウント
    - 季節の観点からデータを見ることができる
  - 日ごとのデータ数カウント
    - 月、年単位でどのように推移しているか見ることができる
    - 横軸が多くなるので折れ線グラフで見た方がいいかも
  - 1時間ごとのデータ数カウント
    - 0~23時のそれぞれのデータ数がわかる
    - つまり1日のうちどの時間帯に集中しているかなどがわかる
  - 曜日ごとのデータ数カウント
    - 週間のデータの推移を見ることができる

<br></br>

## 3. 二変量解析: 深く広く
### 3-1. 「ターゲット(カテゴリ) と 説明(数値)」または「ターゲット(数値) と 説明(カテゴリ)」
- ターゲット変数の水準が少ない場合: ヒストグラム
  - 重ねて表示するのがポイント

  ```python
  # Ageの分布
  sns.distplot(train_age_omit[train_age_omit['Survived']==1]['Age'],kde=True,rug=False,bins=10,label='Survived')
  sns.distplot(train_age_omit[train_age_omit['Survived']==0]['Age'],kde=True,rug=False,bins=10,label='Death')
  plt.legend()
  ```
  
  ![20181216_04](https://user-images.githubusercontent.com/53253817/105465943-ee8e6c00-5cd6-11eb-9308-570109618680.jpeg)

- ターゲット変数の水準が多い場合: 箱ヒゲ図、バイオリンプロット
  - バイオリンプロットは分布図が表示されるので、箱ヒゲよりいいかも

  ```python
  #box plot overallqual/saleprice
  var = 'OverallQual'
  data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)
  f, ax = plt.subplots(figsize=(8, 6))
  fig = sns.boxplot(x=var, y="SalePrice", data=data)
  fig.axis(ymin=0, ymax=800000)
  ```

  ![f3cd6ec84b47032c400f563e80cdca34](https://user-images.githubusercontent.com/53253817/105173918-1231a500-5b65-11eb-8643-08005449ac81.png)

  ```python
  var = 'YearBuilt'
  data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)
  f, ax = plt.subplots(figsize=(16, 8))
  fig = sns.boxplot(x=var, y="SalePrice", data=data)
  fig.axis(ymin=0, ymax=800000)
  plt.xticks(rotation=90)
  ```

  ![3ce436f04ca45fafceefeb757daa9c7b](https://user-images.githubusercontent.com/53253817/105173922-12ca3b80-5b65-11eb-88b1-d15f1b8795bb.png)

- 見るところ
  - 水準ごとにどのような分布をしているか？
  - 水準ごとに特定の数値の範囲だけ違う分布になっている部分はあるか？

### 3-2. ターゲット(カテゴリ) と 説明(カテゴリ)
- 水準の組み合わせが少ない場合: カウントプロット(棒グラフ)
  - 説明変数の水準ごとにターゲット変数のカウントプロットをプロットしてみる

  ```python
  sns.countplot('Sex',hue='Survived',data=df_train)
  ```

  ![2021-01-22 17 10のイメージ](https://user-images.githubusercontent.com/53253817/105464378-da496f80-5cd4-11eb-955a-d6c3f1f6581a.jpeg)

- 水準の組み合わせが多い場合: ヒートマップ
  - カウント数が多ければ多いほど色が濃くなる
  - ここでは相関のヒートマップではないので注意

  ```python
  plt.figure(figsize=(12, 9))
  sns.heatmap(df_flights_pivot, annot=True, fmt='g', cmap='Blues')
  ```

  ![https---qiita-image-store s3 amazonaws com-0-68432-390fc30d-9541-807e-11a0-b10a6412c85d](https://user-images.githubusercontent.com/53253817/105467646-547bf300-5cd9-11eb-85e6-879b3411d61c.png)

- 見るところ
  - 特定の水準だけ極端に多いなどの偏った分布はあるか？

### 3-3. ターゲット(数値) と 説明(数値)
- 散布図

```python
data = pd.concat([df_train["target"], df_train["relation"]], axis=1)
data.plot.scatter(x="relation", y="target", ylim=(0,800000))
```

- 見るところ
  - 正の相関、負の相関があるのかどうか？
  - 相関があるならその強弱は？
  - 外れ値は？
  - 線形的か？指数的か？

### 3-4. その他
- カテゴリをcountplotで出現回数を可視化  
- 数値をdistplotで出現回数を可視化  
- カテゴリ(x軸)×数値(y軸)で可視化していく。pairplotやboxplotで相関や規則性を見る。  
- 数値×数値で可視化していく。pairplotなど。  
### 参考文献  
- 可視化での詳しいメモは後述する「可視化でわかったこと」を参照  
- 可視化は以下の方法から必要なものを選んでいけばいいと思う。  
  - https://www.kaggle.com/kralmachine/seaborn-tutorial-for-beginners  
  - https://www.kaggle.com/kanncaa1/seaborn-tutorial-for-beginners  
  - https://qiita.com/TomoyukiAota/items/fd75c28b802bad9e6632  

<br></br>

## 4. 二変量解析: 浅く広く
- 大まかな手順
  - pairplotを使って二変数間の散布図を確認
    - 外れ値があった場合は相関がおかしくなるので散布図も確認する必要がある
    - 例えば、せっかく相関があるのに、外れ値のせいで相関がないと判定されるケースがある
  - 相関行列により全体の相関性を把握
  - ターゲット変数と相関の強い特徴量に絞る
  - 散布図で関係を確認
### 4-1. 全ての変数の散布図確認
- 外れ値はないか？
  - 外れ値がある場合はメモしておく
  - 外れ値がある場合は相関係数に悪影響を及ぼす可能性があるため
- 非線形な形の散布図はないか？
  - ピアソンの相関係数だと、「線形性」しか判断できないから
  - 意味のある「非線形性」の変数を捨てないようにするため
### 4-2. 全体の相関性の可視化
1. 明らかにターゲット変数と相関が高い(白色で表示されている)変数は何か？
2. 期待が低いと思っていたのに、ターゲット変数との相関が高い変数は何か？
3. なぜその結果になるのかを現実世界の観点から仮説を立てる
  - 説明を求められた時や、新しい特徴量を作成するときに役立つ

```python
corrmat = df_train.corr()
f, ax = plt.subplots(figsize=(12, 9))
sns.heatmap(corrmat, vmax=.8, square=True)
```

### 4-3. ターゲット変数と相関が強い特徴量の可視化
1. ターゲット変数との相関が高いものtop10を以下で抜き出す
  - 下記のコードで表示できる
2. 多重共線性(マルチコ)が発生していないか？
  - 説明変数同士の相関が高い場合に多重共線性が発生していると判断する
3. なぜその結果になるのかを現実世界の観点から仮説を立てる
  - 説明を求められた時や、新しい特徴量を作成するときに役立つ

```python
#saleprice correlation matrix
k = 10 #number of variables for heatmap
cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index
cm = np.corrcoef(df_train[cols].values.T)
sns.set(font_scale=1.25)
hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)
plt.show()
```

- 相関が高いことから重要な変数がわかる
- 多重共線性があるとなぜダメなのか
  - https://yolo-kiyoshi.com/2019/05/27/post-1160/
### 4-4. 散布図で関係を確認
1. おかしな外れ値はないかの最終確認
2. ドットが連なって境界線(上限、下限)を描いていないか？
3. 線形的に増減しているのか？
4. 指数的に増減しているのか？

- 4-2で調べたターゲット変数と相関が高いものtop10から多重共線性が発生しているものを除いた変数間で散布図をプロットする

```python
#scatterplot
sns.set()
cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']
sns.pairplot(df_train[cols], size = 2.5)
plt.show()
```

<br></br>

## 5. スライシング
- カテゴリ変数の水準を軸にして、さらに条件を絞ってプロットしていくこと
- データを様々な軸、様々な水準で区切り、層別にすることによって、全体をぼんやりみるのではなく、意味付けされた層の特徴を理解することが出来る
- やり方
  - 大きな傾向からスライシングしていく
    - 性別ごと -> 年齢ごと -> 性別かつ年齢ごと -> ...
  - カテゴリ変数に注目して、ロジックツリーを作っていくとアイデアが結構出てきそう

![2021-01-22 18 08のイメージ](https://user-images.githubusercontent.com/53253817/105470418-e0434e80-5cdc-11eb-8708-8da027b4c88a.jpeg)


<br></br>

## 6. データクリーニング
### 6-1. 欠損データの処理
```python
#missing data
total = df_train.isnull().sum().sort_values(ascending=False)
percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_data.head(20)
```

- 欠損データを考えるときに重要な項目
  - 欠損データはどの程度あるのか？
  - 欠損データはランダムなのか？、パターンがあるのか？
- この段階で1~5でいらないと判断された変数もまとめて削除する
- 欠損データがあって削除するもの
  - 相関が高かった同士の変数の中で欠損値が少ないものだけを残して、それ以外は削除する
- 欠損データがあっても削除しないもの
  - 「ほんの少ししか欠損データがない」かつ「他の変数と相関がないことから、あったほうがモデルの表現力が上がる可能性があるもの」
    - この場合は欠損しているところの「行」だけを削除するといった方針を取る

### 6-2. 外れ値の処理
1. 単変量解析
  - ターゲット変数のみについての外れ値を見る
2. 二変量解析
  - 散布図を使って、ターゲット変数と特徴量の間の関係をプロットする(前述でやってきた作業の図を使う)
  - 外れ値を目視して削除する
  - 外れ値の定義は感覚

<br></br>

## 7. 多変量解析できるかどうかの確認
- ここからはターゲット変数に対して多変量解析を適用することができるかを確認していく
  - 多変量解析とは、要はモデルを学習して予測させるということ
### 7-1. 検証するべき4つの仮定(多分線形モデルだけ？？)
- 正規性
  - データが正規分布にしたがっているように見えるかどうか
  - したがっていなかったら対数変換をするとか
    - 正の歪度がある場合は対数変換が有効
- 等分散性
  - 全変数において分散が同じレベルであることが望まれる
  - プロットの広がりがどの場所も等しくなることが、等分散性であるということ

  ![https---qiita-image-store s3 amazonaws com-0-247972-9ff48625-5962-c18a-2656-af21493d9b53](https://user-images.githubusercontent.com/53253817/105184231-73ac4080-5b72-11eb-82b4-a73c8d846d9d.jpeg)

- 線形性
  - 散布図を調べて線形パターンになっていることが望まれる
  - パターンが線形でない場合は、データ変換を探索する価値がある
- 相関誤差の欠如
  - 時系列データの場合に注意すべきらしい
  - よくわからん

```python
#transformed histogram and normal probability plot
sns.distplot(df_train['SalePrice'], fit=norm)
fig = plt.figure()
res = stats.probplot(df_train['SalePrice'], plot=plt)
```

<br></br>

## 8. 学習
### 8-1. モデルの選択
- 基本的にGBDTを使用することになる
  - XGBとかLGBMとか
- kaggle本のP230にモデルの選択方法が書いてあるが、基本的にGBDTでどうにかなるっぽい
### 8-2. 交差検証のデータを作成
- K-foldとかtime-splitとか

<br></br>

## 9. 特徴量エンジニアリング
- 8までですある程度のを出すことができた後に、さらにスコアをあげる時に行う
- とりあえずkaggle本の第3章を見ながら進めればいいと思う
- 以下のデータごとの処理方法がまとまっている
  - Numeric
  - Categorical/Ordinal
  - Datetime
  - Coordinates
  - https://ishitonton.hatenablog.com/entry/2019/02/24/184253
- スプレッドシートに
### 9-1. 数値変数の変換
#### 9-1-1. 線形変換: 分布の伸縮
- 標準化
  - 使用する場面
    - 線形回帰やロジスティック回帰などの線形モデルの場合
    - ニューラルネットの場合
    - 二値変数以外
- Min-Maxスケーリング
  - 用途としては標準化と同じだが、外れ値の影響を受けやすいため標準化の方が使用される
#### 9-1-2. 非線形変換: 分布の形状変換
- logx変換
  - 0が含まれていると使用できない
- log(x+1)変換
  - 変数に0が含まれている場合に使用できる
- Box-Cox変換
- Yeo-Johnson変換
- generalized-log-transformation
  - あまり使われていない
- その他
  - 絶対値、平方根、二乗、n乗、
  - 正の値かどうか、ゼロかどうかなどの二値変数
  - 数値の端数を取る
  - 四捨五入、切り上げ、切り捨て
#### 9-1-3. clipping
- 上限下限を設定して、外れ値を上限か下限に丸める
#### 9-1-4. binning
- 数値変数を区間で分割してカテゴリ変数として扱う方法
#### 9-1-5. 順位への変換
- 数値変数から大小関係のみに注力して抽出する方法
- 例えば「10,0,40,50,60,60」というデータがあった場合
  - pandasの場合: 「1, 0, 2, 3, 5.5, 5.5」
    - 同順位が平均される
  - numpyの場合: 「1, 0, 2, 3, 4, 5」
    - 同順位はどちらかが上位になる
#### 9-1-6. Rank-Gauss
- 数値を順位に変換した後に、順位を保ったまま正規分布に近づける手法
- ニューラルネットのでモデルを作成するときに、標準化よりも良い性能を示す
### 9-2. カテゴリ変数の変換
- 変数が文字列でなくても、値の大きさや順序に意味がない場合はカテゴリ変数として扱う
- テストデータにしかない水準の場合
  - 影響が微小の場合はそのまま
  - 最頻値で補完、欠損値としてみなして欠損値を予測する
  - 変換を行うときにその変換における平均値を入れる
#### 9-2-1. one-hot encoding
- n個の水準を持つカテゴリ変数をn個の二値変数に変換する方法
- カテゴリ変数の水準が多すぎる場合は以下の方法を検討
  - 別のencoding
  - カテゴリ変数をグルーピングして水準を減らしてからone-hot encoding
  - 頻度の少ないカテゴリを「その他のカテゴリ」にまとめる
#### 9-2-2. label encoding
- 水準を整数に置き換えるだけ
- GBDTを使う場合は、この方法が一般的
#### 9-2-3. feature hashing
- あまり使われない
#### 9-2-4. frequency encoding
- 出現回数、出現頻度でカテゴリ変数を置き換える方法
- 出現回数とターゲット変数の間に関係がある場合は有効
#### 9-2-5. target encoding
- リークする可能性があるから十分に注意して使う
#### 9-2-6. embedding
- 自然言語処理などで使われる
#### 9-2-7. 順序変数の扱い
- 決定木系のモデルの場合
  - 順序を保ちながらそのまま整数に変換すればいいだけ
- その他のモデル
  - 数値変数と見ることもできるし、カテゴリ変数として見ることもできる
#### 9-2-8. カテゴリ変数の分割
- 型番などの「ABC-10002」などは「ABC」と「10002」に分割する
  - そのままencodingしてしまうと型番の情報が減ってしまうから
### 9-3. 日付・時刻を表す変数の変換
- 年、月、日
- 年月
- 月日
- 曜日
- 祝日
- 特別な日
  - クリスマスなど
- 時
- 分
- 秒
- 時間差
  - 家が建てられてからどれだけ経ったかなど
### 9-4. 変数の組み合わせ
- データの知識を使って、どの組み合わせが効果がありそうかという仮説を立てながら行う
- まずはスプレッドシートのセグメントに書いたものを参考に考える
  - 同じセグメントなら組み合わせるとか加減乗除してみるとか
#### 9-4-1. 数値変数 × カテゴリ変数
- カテゴリ変数の水準ごとに、数値変数の平均や分散といった統計量をとる
- カテゴリ変数の水準ごとに、数値変数の値を絞る
#### 9-4-2. 数値変数 × 数値変数
- 数値変数同士を加減乗除したり、あまりを出したり、二つの値が同じかどうかを特徴量にしたり
- 例えば、物件の面積と部屋数で割るとか
- GBDTは加減より乗除をモデルに反映しづらいから、乗除を特徴量に加えると良かったりする
#### 9-4-3. カテゴリ変数 × カテゴリ変数
- 水準がかなり増えることに注意して使う
#### 9-4-4. 行の統計量をとる
- 欠損値、ゼロ、負の値をカウント
- 平均、分散、最大、最小
### 9-5. 他のテーブルの結合
- kaggleなどで複数のcsvファイルに分割されていたりするから、それを結合する
### 9-6. 集約して統計量をとる: 1対多のデータの時に使用する
- 1対多のデータって何？
  - 例えば、ユーザIDと購入商品は1対多の関係なので、ユーザIDごとにgroupbyして平均やカウントなどの統計量をとる
  - ユーザIDだけでなくて、カテゴリ変数を基準にgroupbyするならなんでも良い
- 方法
  - 単純な統計量をとる
    - レコード数のカウント、ユニーク数のカウント、存在するかどうか、合計・平均・割合、最大・最小・標準偏差・中央値・分位点・尖度・歪度
  - 時間的な統計量をとる
    - 間隔・頻度
    - 直近、最初
    - etc
  - 条件を絞る
    - 集計対象の期間を絞ったり
  - 集計する単位を変える
    - 他のカテゴリ変数を基準にgroupbyして上記のことを行ってみる
    - 例えば、ぶどうを基準にすると、ぶどうを買う人はどのような人たちかが分かったりする
  - 新たなカテゴリを作成する
    - 「ぶどう、イチゴ、レモン」をまとめて「果物」カテゴリを作成したり
- 詳しくはkaggle本P166〜
### 9-7. 時系列データの扱い
- 予測する時点よりも過去のデータだけから作成する。CVの時も注意.
- 過去の値をそのまま使うラグ特徴量が効果的
  - N日前, 移動平均や最大、最小など
  - 過去よりも直近の方が傾向が似ていることを反映して重み付け平均することも考えられる。
  - 自身のラグ特徴量意外に似た傾向を示す他のレコードのラグ特徴量を集約することも
- テストデータ内で参照できる過去データが異なることに注意する
  - テストデータ内の期間毎にモデルを個別に作る方法もある
### 9-8. 次元削減・教師なし学習を使った特徴量
- 全体ではなくて一部の列にだけ適用することも可能
#### 9-8-1. PCA
#### 9-8-2. NMF
#### 9-8-3. LDA
#### 9-8-4. t-SNE
#### 9-8-5. UMAP
#### 9-8-6. オートエンコーダー
- ニューラルネットを使った次元圧縮
#### 9-8- クラスタリング
- クラスタリングして出てきた結果をカテゴリ変数として使う
- または、クラスタ中心からの距離を特徴量に使うこともできる
### 9-10. その他のテクニック
- あとで

<br></br>

## 自分メモ
### 大まかな解析手法
- 単変量解析
  - 1つの変数だけを取り上げて解析すること
  - 平均や中央値といった数字で分析を行うことも可能だが、数字だけを見ていてもわかりにくいので、通常はヒストグラムや箱ひげ図で視覚化して分析する
- 二変量解析
  - 二変数間を解析すること
  - 二変量解析では、通常、相関係数を求めて分析を行うが、視覚化としては散布図、そしてクロス集計表もよく用いられる
- 多変量解析
  - 重回帰分析やクラスタリングといった、3つ以上の変数が絡む解析

![2021-01-20 22 39のイメージ](https://user-images.githubusercontent.com/53253817/105182455-63936180-5b70-11eb-88c1-69d5cd5995b1.jpeg)

![2021-01-20 22 40のイメージ](https://user-images.githubusercontent.com/53253817/105182597-90477900-5b70-11eb-9f73-2ecdb6581977.jpeg)

### ヒートマップの相関だけでなくて散布図もみる理由
- 相関は「線形」の場合を判定しているのであって、「非線形」のものは判定できない
- ヒートマップだけで使える変数を判断してしまうと、規則性のある「非線形」の変数を捨ててしまう可能性がある
- つまり、散布図で見て、規則性のある形をしていないか見ることが重要
- 非線形の規則性を判定する「MIC」という手法もある

### 数値変数とカテゴリカル変数の分類
1. 平均をとることに意味があるかどうか？
  - ある場合: 2に進む
  - ない場合: カテゴリカル変数
2. ユニークな値の数が15未満か？
  - 15未満の場合: カテゴリカル変数
  - 15以上の場合: 数値変数
  - 15以上だけどカテゴリカルに扱いたい場合: 3に進む
3. カテゴリカルにするための処理を加える
  - ビニング
  - 頻度が少ないものをまとめて一つのカテゴリにする
- 1の理由としては、例えば「1型糖尿病」と「2型糖尿病」の平均値をとっても「1.5型糖尿病」という意味をなさない値が出てくるから
- 2の理由としては、「mean, min, max and zeros」といった統計量を取った時に意味をなさない可能性が多いから
  - 統計量は十分な量があって初めて機能する
  - https://github.com/pandas-profiling/pandas-profiling/issues/72
- もしかしたらどっちとして扱ってもいい場合があるかも

### 回帰と分類の区別の仕方
- ターゲット変数が連続値の場合は回帰、離散値の場合は分類
- http://www.mi.u-tokyo.ac.jp/consortium2/pdf/4-8_literacy_level_note.pdf

### 分布型の確認と正規性の検定
- https://qiita.com/uri/items/e656f90e9dda342c54bb

### 正規分布にしたがっているからといって線形性があるとは限らない
- 目的変数が正規分布にしたがっているかは、ヒストグラムを見るとわかる
- 残差が正規分布にしたがっているかは、QQプロットを見るとわかる
- 線形性に関しては、目的変数と説明変数の散布図を見ればわかる
- 散布図は二変数の図なので、正規分布かどうかは読み取ることはできない(当たり前だが)

### 離散型確率変数と連続型確率変数の違い
- 離散型は1,2,3...のように飛び飛びの値
- 連続型は数値と数値の間を無限に取れる値のこと。例えば1と2の間なら1.1とか1.0002とか無限に取れる
- https://detail.chiebukuro.yahoo.co.jp/qa/question_detail/q12119105417
- https://ai-trend.jp/basic-study/basic/discrete-continuous-variable/

### 「モデル」と「仮定している分布」
- ツールボックスアプローチとモデリングによるアプローチ
  - 今まで慣れ親しんでいる機械学習を使ったアプローチはツールボックスアプローチ
  - ベイズ推定などがモデリングによるアプローチ。高度な数学や統計の知識が必要
- 確率変数と確率分布
  - 確率変数: 「コインの裏や表」のこと
  - 確率分布: コインが表になる確率や裏になる確率について記述している分布のこと(確率密度関数を思い浮かべればいい)
- そもそも「仮定している分布」という言葉の意味は何か？
  - ターゲット変数(確率変数)がどのような分布(確率分布)から生成されているデータなのかということ
  - 例えばターゲット変数がベルヌーイ分布にしたがっているのなら、ターゲット変数は0,1の値をとるということ
- 種類
  - 線形回帰(線形モデル): 正規分布(ガウス分布)
  - 正の値しか取らない回帰: ポワソン分布
  - 二値分類: ベルヌーイ分布

### 線形回帰モデル、一般化線形回帰モデル、機械学習

- 線形回帰モデル

![2021-01-22 1 26のイメージ](https://user-images.githubusercontent.com/53253817/105380091-dd028100-5c50-11eb-904e-5f513bae7da2.jpeg)

- 一般化線形回帰モデル

![2021-01-22 1 26のイメージ (1)](https://user-images.githubusercontent.com/53253817/105380104-df64db00-5c50-11eb-89bb-69de65cabb7b.jpeg)

- 機械学習

![2021-01-22 1 26のイメージ (2)](https://user-images.githubusercontent.com/53253817/105380109-dffd7180-5c50-11eb-8e38-1c1546d239f2.jpeg)

- 機械学習の手法例

![2021-01-22 1 28のイメージ](https://user-images.githubusercontent.com/53253817/105380422-32d72900-5c51-11eb-8267-9cd07b87c1d7.jpeg)

- http://www.actuaries.jp/lib/annual/2018-12.pdf

---

### 3-1.groupby系  
・カテゴリ変数でgroupbyをしてから数値変数でmeanやstdを取って、その値で何かの数値変数を割る(もしかしたら四則演算もするのかな？？)  
・groupbyでなんらかの処理をする  
・カテゴリ内の平均  
・
  
### 3-2.その他  
・count,sum,max,min,rollingとかの特徴量作成  
・is_null,is_zero,is_not_zero,over_0.5とかの特徴量作成  
・PolynomialFeaturesでクロスフィーチャーを作る  
・行ごとの統計情報をまとめる(NaNの数,0の数,負の数)  
・「.」や「_」で区切って特徴量にする  
・行ごとにNaNのカウント  
・とりあえず四則演算  
・ビニングでカテゴリカルに  
・trainとtestそれぞれにない値のフラグ  
・NaNとそれ以外の値の特徴量  
・特徴量組み合わせ  
  
### 3-3.外れ値の対処  
・数値変数に対してlog変換やbox-cox変換を使うことで外れ値をならす  
https://www.ten-kara-data.com/how-to-outlier/  
・clipping  
0以下の数値がある時はlog変換できないからこっちを使うといいかも。  

### 3-4.参考文献  
・クロスフィーチャー  
http://www.mirandora.com/?p=2505  
  
## 4.特徴量削除(次元削除を含む)  
### 4-1.自動で次元削除  
・heat_mapで相関が高い変数を抜き出してくる、その変数たちをPCAにかける。  
上の動作を分けた変数ごとに何度も繰り返していく。  
・木モデルはNMFという手法のほうが効く可能性はある。  
  
### 4-2.手動で不要な特徴を削除する段階  
・自作のsum_cols_nanで「ほぼ同じ値(9割)」「ほぼnull(9割)」「値が一つしか無い」カラムを削除する  
・相関係数が1(または0.9以上)のうちどちらかのカラムを削除(PCAをした後にやるべき)  
  
## 5.学習できる形と学習しやすい形に変換  
### 5-1.学習できる形  
・カテゴリ変数を手動で全部リスト化してfor文で全部label encoding(trainとtest合わせて)  
・infと-infをNaNに変換  
  
### 5-2.学習しやすい形  
・目的変数をlog変換やbox-cox変換で正規分布に近づける。ただし、平均値が左側にずれていて、右側に裾が長い分布に対してだけ有効。  
・変換前より変換後の方が正規分布に近付いたかを判断するには「QQプロット」を使った後に「シャピロウィルク検定」を使うことで正確に判断できる。  
  
※説明変数にlog変換を使う場合は外れ値をならすために使ってる。決して正規分布に近づけるわけではない。目的変数に使うと正規分布に近づけるために使っている。  
https://qiita.com/flystaslingan40/items/7fe67fb47d88e4811301  
https://www.ten-kara-data.com/how-to-outlier/  
  
## 6.特徴選択  
・再帰的特徴選択(RFE)で特徴量を減らしていく  
・feature_importanceも活用  
  
## 7.validationの選択  
タスクによって方法が異なるので注意。例えば時系列データにはtimesplitで、k-foldしないといけないとか。  
  
## 8.モデル構築、学習、評価、チューニング  
主にGBDTを使うことになると思う  
チューニングはベイズ最適化を行うといいと思う。  
http://www.mirandora.com/?p=2505  
  
## 9.アンサンブル  
スタッキングやブレンディングなど。  
・単純にK-foldしたそれぞれの結果をaveraging  
・多様性を求めるために色々なモデルでアンサンブル  
・後々に色々な特徴量のパターンで作成したそれぞれのモデルをアンサンブル  
  

## 参考文献
- ieee-cis fraud detection(カーネル1位)
  - https://www.kaggle.com/artgor/eda-and-models
- データ分析の色々な手法  
  - https://qiita.com/TaigoKuriyama/items/8f9286b5c882819adebb#%E5%85%A8%E3%82%AB%E3%83%A9%E3%83%A0%E3%81%AE%E3%83%92%E3%82%B9%E3%83%88%E3%82%B0%E3%83%A9%E3%83%A0%E8%A1%A8%E7%A4%BA  
- コンペに役立つtips
  - https://naotaka1128.hatenadiary.jp/entry/kaggle-compe-tips
- 特徴選択の色々な手法
  - https://qiita.com/shimopino/items/5fee7504c7acf044a521
